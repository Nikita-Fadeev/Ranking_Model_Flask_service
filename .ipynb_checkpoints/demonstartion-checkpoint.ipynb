{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5fd3f3b-2a4b-4b84-b0cf-89778a22be48",
   "metadata": {},
   "source": [
    "В качестве финального проекта реализована система подсказок похожих вопросов на данных сайта Quora. Поиск производится исключительно по основному заголовку без уточняющих деталей.\n",
    "\n",
    "Система представлена микросервисом на основе Flask. Верхнеуровнево пайплайн и критерии можно представить так:\n",
    "\n",
    "Сначала происходит фильтрация запроса по языку (с помощью библиотеки LangDetect) — исключаются все запросы, для которых определённый язык не равняется \"en\". Затем происходит поиск вопросов-кандидатов с помощью FAISS (по схожести векторов) — в этой части предлагается ограничиться векторизацией только тех слов, эмбеддинги которых есть в исходных GLOVE-векторах. Эти кандидаты реранжируются KNRM-моделью, после чего до 10 кандидатов выдаются в качестве ответа.\n",
    "\n",
    "\n",
    "На сервере реализовано две ручки: для запросов (для поиска похожих вопросов) и для создания FAISS-индекса.\n",
    "\n",
    "/query — принимает POST-запрос. Должна вернуть json, где status='FAISS is not initialized!' в случае, если в решение не были загружены вопросы для поиска с помощью второго метода. \n",
    "\n",
    "Формат запроса для query:\n",
    "\n",
    "json-запрос, с единственным ключом 'queries', значение которого — список строк с вопросами (Dict[str, List[str]]).\n",
    "\n",
    "Формат ответа (в случае созданного индекса) — json с двумя полями. lang_check описывает, был ли распознан запрос как английский (List[bool], True/False-значения), suggestions — List[Optional[List[Tuple[str, str]]]].\n",
    "\n",
    "В этом списке для каждого запроса из query необходимо указать список (до 10) найденных схожих вопросов, где каждый вопрос представлен в виде Tuple, в котором первое значение — id текста (см. ниже), второе — сам непредобработанный текст схожего вопроса. Если проверка на язык не пройдена (не английский), либо произошёл какой-то сбой в обработке — оставьте None в списке вместо ответа (например, [[(..., ...), (..., ...), ...], None, ... ]).\n",
    "\n",
    "/update_index — принимает POST-запрос, в котором в json присутствует поле documents, Dict[str,str] — все документы, где ключ — id текста, значение — сам текст. На предобработку и создание индекса даётся 200 секунд. Подразумевается, что инициализация происходит единоразово, поэтому не нужно беспокоиться о повторном вызове этого метода. В возвращаемом json'е должно быть два ключа: status (ok, если всё прошло гладко) и index_size, значение которого — единственное целое число, хранящее количество документов в индексе.\n",
    "\n",
    "В ноутбуке реализована демонстрация работы сервиса. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b51b867-da7e-481b-8d1c-5c17627ca2e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>:root { --jp-notebook-max-width: 95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>:root { --jp-notebook-max-width: 95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df801aa8-9d06-42d5-9b2b-923dcff0e453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc150be6-d44f-4bcd-99a6-f770b2116275",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update Response:  {'status': 'ok'}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'print_into_terminal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 76\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUpdate Response: \u001b[39m\u001b[38;5;124m'\u001b[39m, update_response)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# POST query on server\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m query_response, query \u001b[38;5;241m=\u001b[39m \u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuery Response: \u001b[39m\u001b[38;5;124m'\u001b[39m,  query_response)\n",
      "Cell \u001b[1;32mIn[4], line 54\u001b[0m, in \u001b[0;36mquery\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/query\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     53\u001b[0m payload \u001b[38;5;241m=\u001b[39m sol\u001b[38;5;241m.\u001b[39m_generate_json_server_query()\n\u001b[1;32m---> 54\u001b[0m \u001b[43mprint_into_terminal\u001b[49m(payload[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     56\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(url, data\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mdumps(payload), headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson(), payload\n",
      "\u001b[1;31mNameError\u001b[0m: name 'print_into_terminal' is not defined"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "from typing import Dict, List, Tuple, Union, Callable, Optional\n",
    "import builder \n",
    "# URL вашего Flask приложения\n",
    "base_url = 'http://127.0.0.1:5000'\n",
    "\n",
    "class Solution:\n",
    "  # import QUORA data + debug functions \n",
    "  # generate query, generate data for initial  FAISS indexing\n",
    "  def __init__(\n",
    "      self,\n",
    "      glue_qqp_dir,\n",
    "      ):\n",
    "    self.glue_qqp_dir = glue_qqp_dir\n",
    "    self.glue_df_train = self.get_glue_df('train')\n",
    "    self.glue_df_test = self.get_glue_df('dev')\n",
    "    self.json_query = self._generate_json_server_query()\n",
    "    self.json_documents = self._generate_json_server_documents()\n",
    "\n",
    "  def get_glue_df(self, partition_type: str) -> pd.DataFrame:\n",
    "      assert partition_type in ['dev', 'train']\n",
    "      if pd.__version__ > '1.3.0':\n",
    "        glue_df = pd.read_csv(\n",
    "            self.glue_qqp_dir + f'/{partition_type}.tsv', sep='\\t', on_bad_lines='skip', dtype=object)\n",
    "      else:\n",
    "        glue_df = pd.read_csv(\n",
    "            self.glue_qqp_dir + f'/{partition_type}.tsv', sep='\\t', error_bad_lines=False, dtype=object)\n",
    "      glue_df = glue_df.dropna(axis=0, how='any').reset_index(drop=True)\n",
    "      glue_df_fin = pd.DataFrame({\n",
    "          'id_left': glue_df['qid1'],\n",
    "          'id_right': glue_df['qid2'],\n",
    "          'text_left': glue_df['question1'],\n",
    "          'text_right': glue_df['question2'],\n",
    "          'label': glue_df['is_duplicate'].astype(int)\n",
    "      })\n",
    "      return glue_df_fin\n",
    "\n",
    "  def _generate_json_server_query(self) -> Dict[str, List[str]]:\n",
    "    # generating query for debug\n",
    "    return {'query' : self.glue_df_test['text_left'].sample(10).tolist()}\n",
    "\n",
    "  def _generate_json_server_documents(self) -> Dict[str, Dict[str, List[str]]]:\n",
    "    # generating documents for debug\n",
    "    left_dict = self.glue_df_train[['id_left', 'text_left']].set_index('id_left').to_dict()['text_left']\n",
    "    # right_dict = self.glue_df_train[['id_right', 'text_right']].set_index('id_right').to_dict()\n",
    "    # documents = left_dict.update(right_dict)\n",
    "    return {'documents' : left_dict}   \n",
    "  \n",
    "\n",
    "def query() -> Tuple:\n",
    "    # return random queries from QUORA test.tsv dataset\n",
    "    url = f'{base_url}/query'\n",
    "    payload = sol._generate_json_server_query()\n",
    "    print_into_terminal(payload['query'])\n",
    "\n",
    "    response = requests.post(url, data=json.dumps(payload), headers=headers)\n",
    "    return response.json(), payload\n",
    "\n",
    "def update_index() -> Dict:\n",
    "   # return QUORA train.tsv text documents to init FAISS index\n",
    "   url = f'{base_url}/update_index'\n",
    "\n",
    "   payload = sol._generate_json_server_documents()\n",
    "   response = requests.post(url, data=json.dumps(payload), headers=headers)\n",
    "   return response.json()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # init QUORA dataset\n",
    "    sol = Solution(builder.glue_qqp_dir)\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "    # POST documents on server to init FAISS indexes\n",
    "    update_response = update_index()\n",
    "    print('Update Response: ', update_response)\n",
    "    # POST query on server\n",
    "    query_response, query = query()\n",
    "    print('Query Response: ',  query_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cd62f9-f71f-4fe3-bde1-de9a828f845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display query and ranked by KNRM suggestion from server \n",
    "suggestion_arr = np.array(query_response['suggestions'])[:,:,1]\n",
    "N = suggestion_arr.shape[1] # count ranked documents for each query\n",
    "query_arr =  np.array(query['query']).reshape(-1,1)\n",
    "\n",
    "query_df = pd.DataFrame(np.repeat(query_arr, N), columns = ['query'])\n",
    "suggestion_df = pd.DataFrame(suggestion_arr.reshape(-1), columns = ['suggestions'])\n",
    "\n",
    "pd.concat([query_df, suggestion_df], axis = 1).set_index(['query', 'suggestions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537dd93b-7c2c-4ecd-a8b7-c725c2011656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb0040e-fa13-4f85-89d9-fcdd5fa86d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
